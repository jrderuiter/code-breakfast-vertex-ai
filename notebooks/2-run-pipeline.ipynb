{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be273a47-5ac5-48e7-b30e-9fdd6f1d887d",
   "metadata": {},
   "source": [
    "## Running in Vertex Pipelines\n",
    "\n",
    "To make our model training more reproducible, we would like to run it in an automated pipeline that clearly defines the different steps we need to take (e.g. training and evaluating the model) and captures any produced artifacts (e.g. the trained model). In the Google Cloud, we can use Vertex Pipelines for this purpose. \n",
    "\n",
    "Vertex Pipelines allows you to define pipelines as a graph of containerized tasks, in which each task performs a specific step needed to train/evaluate/deploy a model.\n",
    "\n",
    "Exercise:\n",
    "* Build the docker image\n",
    "* Run the pipeline\n",
    "* Implement the evaluate component\n",
    "* Add an evaluate step to the pipeline\n",
    "* Add a prediction component + step\n",
    "\n",
    "Bonus:\n",
    "* Deploy the trained model as an API\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8c2a0-59ff-450d-a972-e3a98f6216fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p _artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99568b41-54e8-49a7-b589-ad40d486a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_REGION = \"europe-west3\"\n",
    "\n",
    "# Enter your name here. We'll use this to tag your unique\n",
    "# Docker image to avoid clashing with other people.\n",
    "USER_NAME = input(\"Your user name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236f400-a59d-46c2-8caa-98a62265c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! make -C ../ USER_NAME=$USER_NAME docker-push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909875b-f647-4670-a135-4e026d4d4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    Model\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=f\"{GCP_REGION}-docker.pkg.dev/gdd-cb-vertex/docker/fancy-fashion-{USER_NAME}\",\n",
    "    output_component_file=\"_artifacts/train.yaml\",\n",
    ")\n",
    "def train(train_data_path: str, model: Output[Model]) -> None:\n",
    "    \"\"\"Trains the model on the given dataset.\"\"\"\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import joblib\n",
    "    \n",
    "    from fancy_fashion.model import train_model\n",
    "    from fancy_fashion.util import local_gcs_path\n",
    "    \n",
    "    trained_model = train_model(local_gcs_path(train_data_path))\n",
    "\n",
    "    model_dir = Path(model.path)\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(trained_model, model_dir / \"model.pkl\")\n",
    "\n",
    "    \n",
    "@component(\n",
    "    base_image=f\"{GCP_REGION}-docker.pkg.dev/gdd-cb-vertex/docker/fancy-fashion-{USER_NAME}\",\n",
    "    output_component_file=\"_artifacts/evaluate.yaml\",\n",
    ")\n",
    "def evaluate(\n",
    "    test_data_path: str, model: InputPath(\"Model\"), metrics: Output[Metrics]\n",
    ") -> NamedTuple(\"EvalModelOutput\", [(\"roc\", float)]):\n",
    "    # TODO: Implement the actual evaluation.\n",
    "    #       Tip: we can use the evaluate_model function from our package.\n",
    "    metrics.log_metric(\"roc\", 0.9)\n",
    "\n",
    "    \n",
    "@kfp.dsl.pipeline(name=\"fancy-fashion-julian\")\n",
    "def pipeline(train_path: str):\n",
    "    train_task = train(train_path)\n",
    "    \n",
    "    # TODO: Add an evaluate task that uses the evaluate component above.\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"_artifacts/pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f94aef-51a0-46b0-a2b6-2254a22a480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.pipeline_jobs import PipelineJob\n",
    "\n",
    "job = PipelineJob(\n",
    "    display_name=f\"fancy-fashion-{USER_NAME}\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"_artifacts/pipeline.json\",\n",
    "    parameter_values={\n",
    "        \"train_path\": \"gs://gdd-cb-vertex-fashion-inputs/train\"\n",
    "    },\n",
    "    pipeline_root=f\"gs://gdd-cb-vertex-fashion-artifacts/pipelines\",\n",
    "    location=GCP_REGION,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    service_account=f\"vmd-fashion@gdd-cb-vertex.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a8791-abcd-4813-a209-3377884b8010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64babc-8162-4a75-bc09-5a9678fe54fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "poetry-kernel",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
